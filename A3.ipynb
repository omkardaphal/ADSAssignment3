{"cells":[{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["!pip install rdflib"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%fs head 'FileStore/tables/ChatbotDataset.csv'"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"/FileStore/tables/\"\n\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\ncsvSchema = StructType([StructField(\"description\", StringType(), True) ])\n\n# Static DataFrame representing data in the JSON files\nstaticInputDF = (\n  spark\n    .read\n    .schema(csvSchema)\n    .csv(inputPath)\n)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.description)\n           \n    .count()\n)\nstaticCountsDF.cache()\n\n# Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql select description, sum(count) as total_count from static_counts group by description"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(csvSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.description \n      )\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%fs ls FileStore/tables\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["chatbotData = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/ChatbotDataset.csv\")\ndisplay(chatbotData)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["thuss = []\n\na = spark.read.csv(\"/FileStore/tables/ChatbotDataset.csv\", header=True, mode=\"DROPMALFORMED\")\nthuss.append(a)\n\n\ntype(a)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["f = open('/FileStore/tables/ChatbotDataset.csv')\ncsv_f = csv.reader(f)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["!pip install nltk\n!pip install --upgrade nltk\n!pip install -U nltk[corenlp]"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["import nltk\nfrom nltk.stem.lancaster import LancasterStemmer\nimport os\nimport json\nimport datetime\nstemmer = LancasterStemmer()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sh python -m nltk.downloader all"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["import sys\nssplitword = 0\nssplitNegativeWord = 0\ntraining_data = []\nb = []\n\nchatBotDataset = sqlContext.read.format(\"csv\").load(\"/FileStore/tables/ChatbotDataset.csv\")\nfor row in chatBotDataset.rdd.collect():\n for r in row:\n   b.append(r)\n   for w in r.split():\n     if w == 'right' or w == 'better' or w == 'awol' or w == 'like':\n       ssplitword += 1\n     elif w == 'sorry' or w == 'worry':\n       ssplitNegativeWord += 1\n       #print(w)\n   if ssplitword > ssplitNegativeWord:\n       training_data.append({\"class\":\"Positive\", \"sentence\":r})\n   else:\n       training_data.append({\"class\":\"Negative\", \"sentence\":r})\nprint(\"%s sentences in training data\" % len(training_data))  \nprint(training_data)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["words = []\nclasses = []\ndocuments = []\nignore_words = ['?']\n# loop through each sentence in our training data\nfor pattern in training_data:\n   # tokenize each word in the sentence\n   w = nltk.word_tokenize(pattern['sentence'])\n   # add to our words list\n   words.extend(w)\n   # add to documents in our corpus\n   documents.append((w, pattern['class']))\n   # add to our classes list\n   if pattern['class'] not in classes:\n       classes.append(pattern['class'])\n\n# stem and lower each word and remove duplicates\nwords = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\nwords = list(set(words))\n\n# remove duplicates\nclasses = list(set(classes))\n\nprint (len(documents), \"documents\")\nprint (len(classes), \"classes\", classes)\nprint (len(words), \"unique stemmed words\", words)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# create our training data\ntraining = []\noutput = []\n# create an empty array for our output\noutput_empty = [0] * len(classes)\n\n# training set, bag of words for each sentence\nfor doc in documents:\n   # initialize our bag of words\n   bag = []\n   # list of tokenized words for the pattern\n   pattern_words = doc[0]\n   # stem each word\n   pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n   # create our bag of words array\n   for w in words:\n       bag.append(1) if w in pattern_words else bag.append(0)\n\n   training.append(bag)\n   # output is a '0' for each tag and '1' for current tag\n   output_row = list(output_empty)\n   output_row[classes.index(doc[1])] = 1\n   output.append(output_row)\n\nprint (\"# words\", len(words))\nprint (\"# classes\", len(classes))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# sample training/output\ni = 0\nw = documents[i][0]\nprint ([stemmer.stem(word.lower()) for word in w])\nprint (training[i])\nprint (output[i])"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["import numpy as np\nimport time\n\n# compute sigmoid nonlinearity\ndef sigmoid(x):\n   output = 1/(1+np.exp(-x))\n   return output\n\n# convert output of sigmoid function to its derivative\ndef sigmoid_output_to_derivative(output):\n   return output*(1-output)\n\ndef clean_up_sentence(sentence):\n   # tokenize the pattern\n   sentence_words = nltk.word_tokenize(sentence)\n   # stem each word\n   sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n   return sentence_words\n\n# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\ndef bow(sentence, words, show_details=False):\n   # tokenize the pattern\n   sentence_words = clean_up_sentence(sentence)\n   # bag of words\n   bag = [0]*len(words)  \n   for s in sentence_words:\n       for i,w in enumerate(words):\n           if w == s: \n               bag[i] = 1\n               if show_details:\n                   print (\"found in bag: %s\" % w)\n\n   return(np.array(bag))\n\ndef think(sentence, show_details=False):\n   x = bow(sentence.lower(), words, show_details)\n   if show_details:\n       print (\"sentence:\", sentence, \"\\n bow:\", x)\n   # input layer is our bag of words\n   l0 = x\n   # matrix multiplication of input and hidden layer\n   l1 = sigmoid(np.dot(l0, synapse_0))\n   # output layer\n   l2 = sigmoid(np.dot(l1, synapse_1))\n   return l2"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# ANN and Gradient Descent code from https://iamtrask.github.io//2015/07/27/python-network-part2/\ndef train(X, y, hidden_neurons=10, alpha=1, epochs=50000, dropout=False, dropout_percent=0.5):\n\n   print (\"Training with %s neurons, alpha:%s, dropout:%s %s\" % (hidden_neurons, str(alpha), dropout, dropout_percent if dropout else '') )\n   print (\"Input matrix: %sx%s    Output matrix: %sx%s\" % (len(X),len(X[0]),1, len(classes)) )\n   np.random.seed(1)\n\n   last_mean_error = 1\n   # randomly initialize our weights with mean 0\n   synapse_0 = 2*np.random.random((len(X[0]), hidden_neurons)) - 1\n   synapse_1 = 2*np.random.random((hidden_neurons, len(classes))) - 1\n\n   prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n   prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n\n   synapse_0_direction_count = np.zeros_like(synapse_0)\n   synapse_1_direction_count = np.zeros_like(synapse_1)\n       \n   for j in iter(range(epochs+1)):\n\n       # Feed forward through layers 0, 1, and 2\n       layer_0 = X\n       layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n               \n       if(dropout):\n           layer_1 *= np.random.binomial([np.ones((len(X),hidden_neurons))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n\n       layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n\n       # how much did we miss the target value?\n       layer_2_error = y - layer_2\n\n       if (j% 10000) == 0 and j > 5000:\n           # if this 10k iteration's error is greater than the last iteration, break out\n           if np.mean(np.abs(layer_2_error)) < last_mean_error:\n               print (\"delta after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error))) )\n               last_mean_error = np.mean(np.abs(layer_2_error))\n           else:\n               print (\"break:\", np.mean(np.abs(layer_2_error)), \">\", last_mean_error )\n               break\n               \n       # in what direction is the target value?\n       # were we really sure? if so, don't change too much.\n       layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n\n       # how much did each l1 value contribute to the l2 error (according to the weights)?\n       layer_1_error = layer_2_delta.dot(synapse_1.T)\n\n       # in what direction is the target l1?\n       # were we really sure? if so, don't change too much."],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["X = np.array(training)\ny = np.array(output)\n\nstart_time = time.time()\n\ntrain(X, y, hidden_neurons=20, alpha=0.1, epochs=100000, dropout=False, dropout_percent=0.2)\n\nelapsed_time = time.time() - start_time\nprint (\"processing time:\", elapsed_time, \"seconds\")"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# probability threshold\nERROR_THRESHOLD = 0.2\n# load our calculated synapse values\nsynapse_file = 'synapses.json' \nwith open(synapse_file) as data_file: \n   synapse = json.load(data_file) \n   synapse_0 = np.asarray(synapse['synapse0']) \n   synapse_1 = np.asarray(synapse['synapse1'])\n\ndef classify(sentence, show_details=False):\n   results = think(sentence, show_details)\n\n   results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD ] \n   results.sort(key=lambda x: x[1], reverse=True) \n   return_results =[[classes[r[0]],r[1]] for r in results]\n   print (\"%s \\n classification: %s\" % (sentence, return_results))\n   return return_results\n\nclassify(\"like\")\nclassify(\"sorry\")\nclassify(\"better\")\nclassify(\"good\")\n#classify(\"right\")\nprint ()\nclassify(\"right\", show_details=True)"],"metadata":{},"outputs":[],"execution_count":23}],"metadata":{"name":"A3","notebookId":2470462633694023},"nbformat":4,"nbformat_minor":0}
